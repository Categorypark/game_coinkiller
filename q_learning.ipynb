{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] -n NUM_EPISODE [-e EPSILON]\n",
      "                             [-lr LEARNING_RATE]\n",
      "ipykernel_launcher.py: error: the following arguments are required: -n/--num_episode\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\smdkq\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The original code is from https://github.com/dennybritz/reinforcement-learning/tree/master/TD\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from game import Game\n",
    "\n",
    "\n",
    "# In our case, we have 3 action (stay, go-left, go-right)\n",
    "def get_action_num():\n",
    "    return 3\n",
    "\n",
    "\n",
    "## this function return policy function to choose the action based on Q value.\n",
    "def make_policy(Q, epsilon, nA):\n",
    "    \"\"\"\n",
    "    This is the epsilon-greedy policy, which select random actions for some chance (epsilon).\n",
    "    (Check dennybritz's repository for detail)\n",
    "\n",
    "    You may change the policy function for the given task.\n",
    "    \"\"\"\n",
    "    def policy_fn(observation):        \n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        best_action = np.argmax(Q[observation])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn\n",
    "\n",
    "\t\n",
    "## this function return state from given game information.\n",
    "def get_state(counter,score,game_info): # cannot change parameters since game.py is using same parameter\n",
    "    basket_location, item_location = game_info\n",
    "    clock = (3,0,0)# default value when there is no clock in the current state\n",
    "    min_tuple = tuple(item_location[0])\n",
    "    min_height = 9\n",
    "    for items in item_location:\n",
    "        if (abs(3*items[1]+2 - 4*basket_location) <= 8*(9-items[2])) : # exclude the items that we cannot obtain\n",
    "            if( 9 - items[2] <= min_height):\n",
    "                min_height =  9 - items[2]\n",
    "                min_tuple = tuple(items)\n",
    "            elif(items[0] == 2):\n",
    "                clock = tuple(items)\n",
    "\n",
    "    return  (basket_location , min_tuple, clock)\n",
    "\n",
    "\n",
    "## this function return reward from given previous and current score and counter.\n",
    "def get_reward(current_state, action):\n",
    "    basket_location, item , clock = current_state\n",
    "    reward =0\n",
    "\n",
    "    if (clock[0] == 3): # when there is no clock in the state\n",
    "        if action == 1 :\n",
    "            if 3*item[1]+2 < 4*basket_location:\n",
    "                reward = 2\n",
    "            elif 3*item[1]+2 >4*basket_location:\n",
    "                reward = -2\n",
    "        elif action == 2 :\n",
    "            if 3*item[1]+2 > 4*basket_location + 8:\n",
    "                reward = 2\n",
    "            elif 3*item[1] +2 < 4*basket_location+8:\n",
    "                reward = -2\n",
    "    elif (clock[0] == 2) : #when there is a clock , aim is to get the clock not the closest item\n",
    "        if action == 1:\n",
    "            if 3 * clock[1] + 2 < 4 * basket_location:\n",
    "                reward = 3\n",
    "            elif 3 * clock[1] + 2 > 4 * basket_location:\n",
    "                reward = -3\n",
    "        elif action == 2  :\n",
    "            if 3 * clock[1] + 2 > 4 * basket_location + 8:\n",
    "                reward = 3\n",
    "            elif 3 * clock[1] + 2 < 4 * basket_location + 8:\n",
    "                reward = -3\n",
    "\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "def save_q(Q, num_episode, params, filename=\"model_q.pkl\"):\n",
    "    data = {\"num_episode\": num_episode, \"params\": params, \"q_table\": dict(Q)}\n",
    "    with open(filename, \"wb\") as w:\n",
    "        w.write(pickle.dumps(data))\n",
    "\n",
    "        \n",
    "def load_q(filename=\"model_q.pkl\"):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        data = pickle.loads(f.read())\n",
    "        return defaultdict(lambda: np.zeros(3), data[\"q_table\"]), data[\"num_episode\"], data[\"params\"]\n",
    "\n",
    "\n",
    "def q_learning(game, num_episodes, params):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm: Off-policy TD control. Finds the optimal greedy policy\n",
    "    while following an epsilon-greedy policy.\n",
    "    You can edit those parameters, please speficy your changes in the report.\n",
    "    \n",
    "    Args:\n",
    "        game: Coin drop game environment.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "        Q: the optimal action-value function, a dictionary mapping state -> action values.\n",
    "    \"\"\"\n",
    "    \n",
    "    epsilon, alpha, discount_factor = params\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.zeros(get_action_num()))\n",
    "\n",
    "    # The policy we're following\n",
    "    policy = make_policy(Q, epsilon, get_action_num())\n",
    "\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Reset the environment and pick the first action\n",
    "        _, counter, score, game_info = game.reset()\n",
    "        state = get_state(counter,score,game_info)\n",
    "        action = 0\n",
    "        \n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "            current_state = get_state(counter,score,game_info)\n",
    "\n",
    "            # Take a step\n",
    "            action_probs = policy(get_state(counter,score,game_info))\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            done, next_counter, next_score, game_info = game.step(action)\n",
    "            \n",
    "            next_state = get_state(counter,score,game_info)\n",
    "            reward = get_reward(current_state, action) # originally game_info\n",
    "            \n",
    "            counter = next_counter\n",
    "            score = next_score\n",
    "            \n",
    "            \"\"\"\n",
    "            this code performs TD Update. (Update Q value)\n",
    "            You may change this part for the given task.\n",
    "            \"\"\"\n",
    "            best_next_action = np.argmax(Q[next_state])    \n",
    "            td_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "            td_delta = td_target - Q[state][action]\n",
    "            Q[state][action] += alpha * td_delta\n",
    "\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "        \n",
    "        # Print out which episode we're on, useful for debugging.\n",
    "        if (i_episode + 1) % 100 == 0 :\n",
    "            print(\"Episode {}/{} (Score: {})\\n\".format(i_episode + 1, num_episodes, score), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    return Q\n",
    "\n",
    "def train(num_episodes, params):\n",
    "    g = Game(False)\n",
    "    Q = q_learning(g, num_episodes, params)\n",
    "    return Q\n",
    "\n",
    "\n",
    "## This function will be called in the game.py\n",
    "def get_action(Q, counter, score, game_info, params):\n",
    "    epsilon = params[0]\n",
    "    policy = make_policy(Q, epsilon, 3)\n",
    "    action_probs = policy(get_state(counter, score, game_info))\n",
    "    action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "    return action\n",
    "\n",
    "def main():\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-n\", \"--num_episode\", help=\"# of the episode (size of training data)\",\n",
    "                    type=int, required=True)\n",
    "    parser.add_argument(\"-e\", \"--epsilon\", help=\"the probability of random movement, 0~1\",\n",
    "                    type=float, default=0.1)\n",
    "    parser.add_argument(\"-lr\", \"--learning_rate\", help=\"learning rate of training\",\n",
    "                    type=float, default=0.1)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.num_episode is None:\n",
    "        parser.print_help()\n",
    "        exit(1)\n",
    "\n",
    "    # you can pass your parameter as list or dictionary.\n",
    "    # fix corresponding parts if you want to change the parameters\n",
    "    \n",
    "    num_episodes = args.num_episode\n",
    "    epsilon = args.epsilon\n",
    "    learning_rate = args.learning_rate\n",
    "    \n",
    "    Q = train(num_episodes, [epsilon, learning_rate, 0.9])\n",
    "    save_q(Q, num_episodes, [epsilon, learning_rate, 0.9])\n",
    "    \n",
    "    #Q, n, params = load_q()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
